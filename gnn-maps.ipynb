{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN-MAPS: Graph Neural Networks for Spatial Proteomics\n",
    "\n",
    "**Improving Cell Type Classification with Spatial Context**\n",
    "\n",
    "This notebook implements Graph Neural Networks (GNNs) to enhance the MAPS baseline by incorporating cellular neighborhood information.\n",
    "\n",
    "## Setup Instructions for Kaggle:\n",
    "\n",
    "1. **Run Cell 1:** Installs PyTorch Geometric (auto-detects your CUDA version)\n",
    "2. **Run Cell 2:** Imports all libraries and verifies installation\n",
    "3. **Upload Data:** Make sure your cHL_CODEX_annotation.csv is uploaded to Kaggle\n",
    "\n",
    "## What This Notebook Does:\n",
    "- ✅ Builds a K-Nearest Neighbor graph from cell spatial coordinates\n",
    "- ✅ Implements GraphSAGE (memory-efficient GNN)\n",
    "- ✅ Uses NeighborLoader to prevent GPU OOM on limited hardware\n",
    "- ✅ Implements spatial split to prevent data leakage\n",
    "- ✅ Compares GNN performance against baseline MLP\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:59:25.408735Z",
     "iopub.status.busy": "2026-01-16T15:59:25.408307Z",
     "iopub.status.idle": "2026-01-16T15:59:39.661456Z",
     "shell.execute_reply": "2026-01-16T15:59:39.659604Z",
     "shell.execute_reply.started": "2026-01-16T15:59:25.408692Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: False\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\n",
      "Installing PyG extensions for PyTorch 2.8.0 and CUDA cpu...\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.6/645.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\n",
      "✅ PyTorch Geometric installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch Geometric and its dependencies (Kaggle-compatible)\n",
    "# Kaggle already has PyTorch installed, we just need PyG and its extensions\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Install PyTorch Geometric based on the installed PyTorch version\n",
    "!pip install -q torch-geometric\n",
    "\n",
    "# Install extension libraries (torch-scatter, torch-sparse, etc.)\n",
    "# These need to match your PyTorch and CUDA versions\n",
    "import torch\n",
    "pytorch_version = torch.__version__.split('+')[0]\n",
    "cuda_version = torch.version.cuda.replace('.', '') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"\\nInstalling PyG extensions for PyTorch {pytorch_version} and CUDA {cuda_version}...\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    !pip install -q torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-{pytorch_version}+cu{cuda_version}.html\n",
    "else:\n",
    "    !pip install -q torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-{pytorch_version}+cpu.html\n",
    "\n",
    "print(\"\\n✅ PyTorch Geometric installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:59:39.664745Z",
     "iopub.status.busy": "2026-01-16T15:59:39.663681Z",
     "iopub.status.idle": "2026-01-16T15:59:50.752617Z",
     "shell.execute_reply": "2026-01-16T15:59:50.750932Z",
     "shell.execute_reply.started": "2026-01-16T15:59:39.664696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PyTorch Geometric is working correctly!\n",
      "   Test layer created: SAGEConv(16, 32, aggr=mean)\n"
     ]
    }
   ],
   "source": [
    "# Quick verification test - Run this to confirm PyG is working\n",
    "try:\n",
    "    import torch\n",
    "    from torch_geometric.nn import SAGEConv\n",
    "    \n",
    "    # Test if we can create a simple layer\n",
    "    test_conv = SAGEConv(16, 32)\n",
    "    print(\"✅ PyTorch Geometric is working correctly!\")\n",
    "    print(f\"   Test layer created: {test_conv}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Please re-run the installation cell above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-16T15:59:50.754660Z",
     "iopub.status.busy": "2026-01-16T15:59:50.754098Z",
     "iopub.status.idle": "2026-01-16T15:59:52.140115Z",
     "shell.execute_reply": "2026-01-16T15:59:52.138593Z",
     "shell.execute_reply.started": "2026-01-16T15:59:50.754615Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/chl-codex-annotated/cHL_CODEX_annotation.csv\n",
      "\n",
      "✅ PyTorch 2.8.0+cu126 imported successfully!\n",
      "✅ All libraries loaded successfully!\n",
      "   - PyTorch version: 2.8.0+cu126\n",
      "   - CUDA available: False\n",
      "   - Device: CPU\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# List input files (Kaggle-specific)\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Import PyTorch and PyTorch Geometric\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.data import Data\n",
    "    from torch_geometric.loader import NeighborLoader\n",
    "    from torch_geometric.nn import SAGEConv, GATConv\n",
    "    print(f\"\\n✅ PyTorch {torch.__version__} imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import Error: {e}\")\n",
    "    print(\"Please run the first cell to install PyTorch Geometric!\")\n",
    "    raise\n",
    "\n",
    "# Import scikit-learn utilities\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"✅ All libraries loaded successfully!\")\n",
    "print(f\"   - PyTorch version: {torch.__version__}\")\n",
    "print(f\"   - CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"   - Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Defining the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:59:52.142952Z",
     "iopub.status.busy": "2026-01-16T15:59:52.142009Z",
     "iopub.status.idle": "2026-01-16T15:59:54.629070Z",
     "shell.execute_reply": "2026-01-16T15:59:54.627962Z",
     "shell.execute_reply.started": "2026-01-16T15:59:52.142841Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cellLabel</th>\n",
       "      <th>X_cent</th>\n",
       "      <th>Y_cent</th>\n",
       "      <th>cellSize</th>\n",
       "      <th>cellType</th>\n",
       "      <th>BCL.2</th>\n",
       "      <th>CCR6</th>\n",
       "      <th>CD11b</th>\n",
       "      <th>CD11c</th>\n",
       "      <th>CD15</th>\n",
       "      <th>CD16</th>\n",
       "      <th>CD162</th>\n",
       "      <th>CD163</th>\n",
       "      <th>CD2</th>\n",
       "      <th>CD20</th>\n",
       "      <th>CD206</th>\n",
       "      <th>CD25</th>\n",
       "      <th>CD30</th>\n",
       "      <th>CD31</th>\n",
       "      <th>CD4</th>\n",
       "      <th>CD44</th>\n",
       "      <th>CD45RA</th>\n",
       "      <th>CD45RO</th>\n",
       "      <th>CD45</th>\n",
       "      <th>CD5</th>\n",
       "      <th>CD56</th>\n",
       "      <th>CD57</th>\n",
       "      <th>CD68</th>\n",
       "      <th>CD69</th>\n",
       "      <th>CD7</th>\n",
       "      <th>CD8</th>\n",
       "      <th>Collagen.4</th>\n",
       "      <th>Cytokeratin</th>\n",
       "      <th>DAPI.01</th>\n",
       "      <th>EGFR</th>\n",
       "      <th>FoxP3</th>\n",
       "      <th>Granzyme.B</th>\n",
       "      <th>HLA.DR</th>\n",
       "      <th>IDO.1</th>\n",
       "      <th>LAG.3</th>\n",
       "      <th>MCT</th>\n",
       "      <th>MMP.9</th>\n",
       "      <th>MUC.1</th>\n",
       "      <th>PD.1</th>\n",
       "      <th>PD.L1</th>\n",
       "      <th>Podoplanin</th>\n",
       "      <th>T.bet</th>\n",
       "      <th>TCR.g.d</th>\n",
       "      <th>TCRb</th>\n",
       "      <th>Tim.3</th>\n",
       "      <th>VISA</th>\n",
       "      <th>Vimentin</th>\n",
       "      <th>a.SMA</th>\n",
       "      <th>b.Catenin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>B</td>\n",
       "      <td>0.792642</td>\n",
       "      <td>0.617038</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.097688</td>\n",
       "      <td>0.007772</td>\n",
       "      <td>0.005356</td>\n",
       "      <td>0.159593</td>\n",
       "      <td>0.754060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102301</td>\n",
       "      <td>0.104273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054377</td>\n",
       "      <td>0.761659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.742815</td>\n",
       "      <td>0.131788</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.107275</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.166675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.115538</td>\n",
       "      <td>0.808093</td>\n",
       "      <td>0.461070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.781303</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.017596</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.273973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059137</td>\n",
       "      <td>0.091339</td>\n",
       "      <td>0.114237</td>\n",
       "      <td>0.241829</td>\n",
       "      <td>0.681727</td>\n",
       "      <td>0.159092</td>\n",
       "      <td>0.006533</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.139826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1767</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>DC</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127394</td>\n",
       "      <td>0.062173</td>\n",
       "      <td>0.660572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172885</td>\n",
       "      <td>0.006180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053359</td>\n",
       "      <td>0.008472</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.107943</td>\n",
       "      <td>0.016752</td>\n",
       "      <td>0.020134</td>\n",
       "      <td>0.026217</td>\n",
       "      <td>0.333890</td>\n",
       "      <td>0.050817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011369</td>\n",
       "      <td>0.039442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097343</td>\n",
       "      <td>0.025035</td>\n",
       "      <td>0.091633</td>\n",
       "      <td>0.484774</td>\n",
       "      <td>0.110559</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.380801</td>\n",
       "      <td>0.020411</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>0.010586</td>\n",
       "      <td>0.163511</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.010475</td>\n",
       "      <td>0.231264</td>\n",
       "      <td>0.058234</td>\n",
       "      <td>0.111610</td>\n",
       "      <td>0.182849</td>\n",
       "      <td>0.899717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.267198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2545</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>NK</td>\n",
       "      <td>0.060620</td>\n",
       "      <td>0.134818</td>\n",
       "      <td>0.298522</td>\n",
       "      <td>0.083348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182152</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.394073</td>\n",
       "      <td>0.070417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022717</td>\n",
       "      <td>0.059562</td>\n",
       "      <td>0.233413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.456157</td>\n",
       "      <td>0.425833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.699781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.115538</td>\n",
       "      <td>0.425240</td>\n",
       "      <td>0.350698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024695</td>\n",
       "      <td>0.047437</td>\n",
       "      <td>0.394926</td>\n",
       "      <td>0.167511</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>0.406332</td>\n",
       "      <td>0.110243</td>\n",
       "      <td>0.170917</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.033671</td>\n",
       "      <td>0.201099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.454174</td>\n",
       "      <td>0.069180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2899</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>Monocyte</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012896</td>\n",
       "      <td>0.287698</td>\n",
       "      <td>0.268121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124361</td>\n",
       "      <td>0.016805</td>\n",
       "      <td>0.208045</td>\n",
       "      <td>0.037673</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.226853</td>\n",
       "      <td>0.389436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.009068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064621</td>\n",
       "      <td>0.021957</td>\n",
       "      <td>0.234302</td>\n",
       "      <td>0.020596</td>\n",
       "      <td>0.009104</td>\n",
       "      <td>0.331627</td>\n",
       "      <td>0.137570</td>\n",
       "      <td>0.088045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100139</td>\n",
       "      <td>0.844622</td>\n",
       "      <td>0.352126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.057845</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.293497</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.157534</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069358</td>\n",
       "      <td>0.052713</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052230</td>\n",
       "      <td>0.148087</td>\n",
       "      <td>0.228221</td>\n",
       "      <td>0.053541</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.283151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3657</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>CD4</td>\n",
       "      <td>0.201057</td>\n",
       "      <td>0.110590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.745316</td>\n",
       "      <td>0.013513</td>\n",
       "      <td>0.292691</td>\n",
       "      <td>0.039192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207973</td>\n",
       "      <td>0.055012</td>\n",
       "      <td>0.032742</td>\n",
       "      <td>0.458688</td>\n",
       "      <td>0.047203</td>\n",
       "      <td>0.069221</td>\n",
       "      <td>0.269690</td>\n",
       "      <td>0.214097</td>\n",
       "      <td>0.095327</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086232</td>\n",
       "      <td>0.021121</td>\n",
       "      <td>0.705442</td>\n",
       "      <td>0.019364</td>\n",
       "      <td>0.045897</td>\n",
       "      <td>0.027888</td>\n",
       "      <td>0.603429</td>\n",
       "      <td>0.249414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.191781</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.105133</td>\n",
       "      <td>0.026865</td>\n",
       "      <td>0.011771</td>\n",
       "      <td>0.124955</td>\n",
       "      <td>0.154981</td>\n",
       "      <td>0.597647</td>\n",
       "      <td>0.013284</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.234038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145156</th>\n",
       "      <td>152811</td>\n",
       "      <td>2882</td>\n",
       "      <td>8004</td>\n",
       "      <td>119</td>\n",
       "      <td>Neutrophil</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523642</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.182044</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055458</td>\n",
       "      <td>0.019692</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>0.064028</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>0.164975</td>\n",
       "      <td>0.217214</td>\n",
       "      <td>0.051984</td>\n",
       "      <td>0.122875</td>\n",
       "      <td>0.019082</td>\n",
       "      <td>0.122574</td>\n",
       "      <td>0.052267</td>\n",
       "      <td>0.043923</td>\n",
       "      <td>0.359459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.121433</td>\n",
       "      <td>0.122698</td>\n",
       "      <td>0.330551</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.076633</td>\n",
       "      <td>0.029207</td>\n",
       "      <td>0.641434</td>\n",
       "      <td>0.867490</td>\n",
       "      <td>0.796590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074084</td>\n",
       "      <td>0.010816</td>\n",
       "      <td>0.040646</td>\n",
       "      <td>0.054899</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.582192</td>\n",
       "      <td>0.241425</td>\n",
       "      <td>0.063518</td>\n",
       "      <td>0.205187</td>\n",
       "      <td>0.098993</td>\n",
       "      <td>0.790734</td>\n",
       "      <td>0.878804</td>\n",
       "      <td>0.067239</td>\n",
       "      <td>0.082646</td>\n",
       "      <td>0.671506</td>\n",
       "      <td>0.455424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145157</th>\n",
       "      <td>152812</td>\n",
       "      <td>2893</td>\n",
       "      <td>8004</td>\n",
       "      <td>82</td>\n",
       "      <td>Neutrophil</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210238</td>\n",
       "      <td>0.313372</td>\n",
       "      <td>0.076860</td>\n",
       "      <td>0.986128</td>\n",
       "      <td>0.587575</td>\n",
       "      <td>0.044324</td>\n",
       "      <td>0.053227</td>\n",
       "      <td>0.099109</td>\n",
       "      <td>0.090999</td>\n",
       "      <td>0.068262</td>\n",
       "      <td>0.161514</td>\n",
       "      <td>0.180206</td>\n",
       "      <td>0.018674</td>\n",
       "      <td>0.138628</td>\n",
       "      <td>0.151270</td>\n",
       "      <td>0.123264</td>\n",
       "      <td>0.093079</td>\n",
       "      <td>0.090329</td>\n",
       "      <td>0.363216</td>\n",
       "      <td>0.665549</td>\n",
       "      <td>0.225986</td>\n",
       "      <td>0.088162</td>\n",
       "      <td>0.417833</td>\n",
       "      <td>0.076197</td>\n",
       "      <td>0.044858</td>\n",
       "      <td>0.040334</td>\n",
       "      <td>0.940239</td>\n",
       "      <td>0.358436</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003821</td>\n",
       "      <td>0.067585</td>\n",
       "      <td>0.026161</td>\n",
       "      <td>0.034304</td>\n",
       "      <td>0.061233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568493</td>\n",
       "      <td>0.253298</td>\n",
       "      <td>0.078119</td>\n",
       "      <td>0.211576</td>\n",
       "      <td>0.143761</td>\n",
       "      <td>0.801359</td>\n",
       "      <td>0.500603</td>\n",
       "      <td>0.069040</td>\n",
       "      <td>0.077637</td>\n",
       "      <td>0.327586</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145158</th>\n",
       "      <td>152813</td>\n",
       "      <td>2327</td>\n",
       "      <td>8004</td>\n",
       "      <td>92</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620555</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.025792</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.255168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.829552</td>\n",
       "      <td>0.068726</td>\n",
       "      <td>0.109608</td>\n",
       "      <td>0.226921</td>\n",
       "      <td>0.421425</td>\n",
       "      <td>0.067756</td>\n",
       "      <td>0.031506</td>\n",
       "      <td>0.093115</td>\n",
       "      <td>0.235259</td>\n",
       "      <td>0.019809</td>\n",
       "      <td>0.403973</td>\n",
       "      <td>0.788639</td>\n",
       "      <td>0.352180</td>\n",
       "      <td>0.212372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.916077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.418637</td>\n",
       "      <td>0.195219</td>\n",
       "      <td>0.717833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.105218</td>\n",
       "      <td>0.123874</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.712401</td>\n",
       "      <td>0.116814</td>\n",
       "      <td>0.332249</td>\n",
       "      <td>0.051137</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.752380</td>\n",
       "      <td>0.806268</td>\n",
       "      <td>0.250223</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.462049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145159</th>\n",
       "      <td>152814</td>\n",
       "      <td>3371</td>\n",
       "      <td>8003</td>\n",
       "      <td>184</td>\n",
       "      <td>CD4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099606</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.370193</td>\n",
       "      <td>0.012604</td>\n",
       "      <td>0.032381</td>\n",
       "      <td>0.571802</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.092682</td>\n",
       "      <td>0.260991</td>\n",
       "      <td>0.449181</td>\n",
       "      <td>0.018295</td>\n",
       "      <td>0.358866</td>\n",
       "      <td>0.107510</td>\n",
       "      <td>0.549397</td>\n",
       "      <td>0.214797</td>\n",
       "      <td>0.976113</td>\n",
       "      <td>0.878399</td>\n",
       "      <td>0.294202</td>\n",
       "      <td>0.251035</td>\n",
       "      <td>0.023310</td>\n",
       "      <td>0.652705</td>\n",
       "      <td>0.180385</td>\n",
       "      <td>0.015327</td>\n",
       "      <td>0.068150</td>\n",
       "      <td>0.482072</td>\n",
       "      <td>0.395336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018196</td>\n",
       "      <td>0.127575</td>\n",
       "      <td>0.076103</td>\n",
       "      <td>0.071791</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.636986</td>\n",
       "      <td>0.364116</td>\n",
       "      <td>0.125940</td>\n",
       "      <td>0.302916</td>\n",
       "      <td>0.138358</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899182</td>\n",
       "      <td>0.362010</td>\n",
       "      <td>0.142534</td>\n",
       "      <td>0.086661</td>\n",
       "      <td>0.963140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145160</th>\n",
       "      <td>152815</td>\n",
       "      <td>7166</td>\n",
       "      <td>8006</td>\n",
       "      <td>53</td>\n",
       "      <td>M2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083704</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946296</td>\n",
       "      <td>0.010143</td>\n",
       "      <td>0.013962</td>\n",
       "      <td>0.479674</td>\n",
       "      <td>0.173538</td>\n",
       "      <td>0.195209</td>\n",
       "      <td>0.056148</td>\n",
       "      <td>0.197953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020467</td>\n",
       "      <td>0.052267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400651</td>\n",
       "      <td>0.310973</td>\n",
       "      <td>0.291875</td>\n",
       "      <td>0.011798</td>\n",
       "      <td>0.313247</td>\n",
       "      <td>0.017815</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.011127</td>\n",
       "      <td>0.059761</td>\n",
       "      <td>0.250892</td>\n",
       "      <td>0.244636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015597</td>\n",
       "      <td>0.113680</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.397260</td>\n",
       "      <td>0.327177</td>\n",
       "      <td>0.027743</td>\n",
       "      <td>0.203009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.662786</td>\n",
       "      <td>0.307816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373048</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.110449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145161 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cellLabel  X_cent  Y_cent  cellSize    cellType     BCL.2      CCR6  \\\n",
       "0               1      75       2        42           B  0.792642  0.617038   \n",
       "1               2    1767       2        63          DC  0.000000  0.127394   \n",
       "2               3    2545       2        29          NK  0.060620  0.134818   \n",
       "3               4    2899       3        64    Monocyte  0.000000  0.012896   \n",
       "4               5    3657       3        61         CD4  0.201057  0.110590   \n",
       "...           ...     ...     ...       ...         ...       ...       ...   \n",
       "145156     152811    2882    8004       119  Neutrophil  0.000000  0.523642   \n",
       "145157     152812    2893    8004        82  Neutrophil  0.000000  0.210238   \n",
       "145158     152813    2327    8004        92       Other  0.000000  0.620555   \n",
       "145159     152814    3371    8003       184         CD4  0.000000  0.597890   \n",
       "145160     152815    7166    8006        53          M2  0.000000  0.030481   \n",
       "\n",
       "           CD11b     CD11c      CD15      CD16     CD162     CD163       CD2  \\\n",
       "0       0.000089  0.000040  0.000029  0.097688  0.007772  0.005356  0.159593   \n",
       "1       0.062173  0.660572  0.000000  0.000000  0.172885  0.006180  0.000000   \n",
       "2       0.298522  0.083348  0.000000  0.182152  0.000210  0.001401  0.000000   \n",
       "3       0.287698  0.268121  0.000000  0.124361  0.016805  0.208045  0.037673   \n",
       "4       0.000000  0.000000  0.000000  0.000000  0.745316  0.013513  0.292691   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "145156  1.000000  0.182044  1.000000  1.000000  0.055458  0.019692  0.004761   \n",
       "145157  0.313372  0.076860  0.986128  0.587575  0.044324  0.053227  0.099109   \n",
       "145158  0.000089  0.025792  0.000029  0.255168  0.000000  0.000000  0.829552   \n",
       "145159  0.000000  0.099606  0.000029  0.370193  0.012604  0.032381  0.571802   \n",
       "145160  0.000000  0.083704  0.000058  1.000000  0.000000  0.946296  0.010143   \n",
       "\n",
       "            CD20     CD206      CD25      CD30      CD31       CD4      CD44  \\\n",
       "0       0.754060  0.000000  0.102301  0.104273  0.000000  0.000000  0.054377   \n",
       "1       0.000000  0.053359  0.008472  0.000750  0.107943  0.016752  0.020134   \n",
       "2       0.000000  0.394073  0.070417  0.000000  0.010536  0.000000  0.022717   \n",
       "3       0.000019  0.226853  0.389436  0.000000  0.000063  0.009068  0.000000   \n",
       "4       0.039192  0.000000  0.207973  0.055012  0.032742  0.458688  0.047203   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "145156  0.064028  0.018412  0.164975  0.217214  0.051984  0.122875  0.019082   \n",
       "145157  0.090999  0.068262  0.161514  0.180206  0.018674  0.138628  0.151270   \n",
       "145158  0.068726  0.109608  0.226921  0.421425  0.067756  0.031506  0.093115   \n",
       "145159  1.000000  0.092682  0.260991  0.449181  0.018295  0.358866  0.107510   \n",
       "145160  0.013962  0.479674  0.173538  0.195209  0.056148  0.197953  0.000000   \n",
       "\n",
       "          CD45RA    CD45RO      CD45       CD5      CD56      CD57      CD68  \\\n",
       "0       0.761659  0.000000  0.742815  0.131788  0.000479  0.107275  0.000072   \n",
       "1       0.026217  0.333890  0.050817  0.000000  0.859607  0.000000  0.011369   \n",
       "2       0.059562  0.233413  0.000000  0.000000  0.456157  0.425833  0.000000   \n",
       "3       0.064621  0.021957  0.234302  0.020596  0.009104  0.331627  0.137570   \n",
       "4       0.069221  0.269690  0.214097  0.095327  0.000479  0.000000  0.086232   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "145156  0.122574  0.052267  0.043923  0.359459  1.000000  0.121433  0.122698   \n",
       "145157  0.123264  0.093079  0.090329  0.363216  0.665549  0.225986  0.088162   \n",
       "145158  0.235259  0.019809  0.403973  0.788639  0.352180  0.212372  0.000000   \n",
       "145159  0.549397  0.214797  0.976113  0.878399  0.294202  0.251035  0.023310   \n",
       "145160  0.020467  0.052267  0.000000  0.400651  0.310973  0.291875  0.011798   \n",
       "\n",
       "            CD69       CD7       CD8  Collagen.4  Cytokeratin   DAPI.01  \\\n",
       "0       0.166675  0.000000  0.000000    0.001391     0.115538  0.808093   \n",
       "1       0.039442  0.000000  0.097343    0.025035     0.091633  0.484774   \n",
       "2       0.699781  0.000000  0.000000    0.001391     0.115538  0.425240   \n",
       "3       0.088045  0.000000  0.000000    0.100139     0.844622  0.352126   \n",
       "4       0.021121  0.705442  0.019364    0.045897     0.027888  0.603429   \n",
       "...          ...       ...       ...         ...          ...       ...   \n",
       "145156  0.330551  0.037360  0.076633    0.029207     0.641434  0.867490   \n",
       "145157  0.417833  0.076197  0.044858    0.040334     0.940239  0.358436   \n",
       "145158  0.916077  0.000000  0.000075    0.418637     0.195219  0.717833   \n",
       "145159  0.652705  0.180385  0.015327    0.068150     0.482072  0.395336   \n",
       "145160  0.313247  0.017815  0.000299    0.011127     0.059761  0.250892   \n",
       "\n",
       "            EGFR     FoxP3  Granzyme.B    HLA.DR     IDO.1     LAG.3  \\\n",
       "0       0.461070  0.000000    0.001300  0.781303  0.000865  0.017596   \n",
       "1       0.110559  0.000000    0.001300  1.000000  0.380801  0.020411   \n",
       "2       0.350698  0.000000    0.024695  0.047437  0.394926  0.167511   \n",
       "3       0.000000  0.000000    0.003899  0.057845  0.002306  0.293497   \n",
       "4       0.249414  0.000000    0.324929  0.000000  0.000288  0.000000   \n",
       "...          ...       ...         ...       ...       ...       ...   \n",
       "145156  0.796590  0.000000    0.074084  0.010816  0.040646  0.054899   \n",
       "145157  1.000000  0.003821    0.067585  0.026161  0.034304  0.061233   \n",
       "145158  1.000000  0.000000    0.002599  0.002265  0.105218  0.123874   \n",
       "145159  1.000000  0.000000    0.018196  0.127575  0.076103  0.071791   \n",
       "145160  0.244636  0.000000    0.015597  0.113680  0.011819  0.003519   \n",
       "\n",
       "             MCT     MMP.9     MUC.1      PD.1     PD.L1  Podoplanin  \\\n",
       "0       0.000571  0.273973  0.000000  0.059137  0.091339    0.114237   \n",
       "1       0.000571  0.020548  0.005277  0.010586  0.163511    0.000193   \n",
       "2       0.000571  0.006849  0.406332  0.110243  0.170917    0.000193   \n",
       "3       0.000571  0.157534  1.000000  0.069358  0.052713    0.005982   \n",
       "4       0.000571  0.191781  0.001319  0.105133  0.026865    0.011771   \n",
       "...          ...       ...       ...       ...       ...         ...   \n",
       "145156  0.000571  0.582192  0.241425  0.063518  0.205187    0.098993   \n",
       "145157  0.000000  0.568493  0.253298  0.078119  0.211576    0.143761   \n",
       "145158  0.000571  1.000000  0.712401  0.116814  0.332249    0.051137   \n",
       "145159  0.000571  0.636986  0.364116  0.125940  0.302916    0.138358   \n",
       "145160  0.001143  0.397260  0.327177  0.027743  0.203009    0.000000   \n",
       "\n",
       "           T.bet   TCR.g.d      TCRb     Tim.3      VISA  Vimentin  a.SMA  \\\n",
       "0       0.241829  0.681727  0.159092  0.006533  0.000454  0.139826    0.0   \n",
       "1       0.010475  0.231264  0.058234  0.111610  0.182849  0.899717    0.0   \n",
       "2       0.033671  0.201099  0.000000  0.004900  0.454174  0.069180    0.0   \n",
       "3       0.000000  0.000000  0.052230  0.148087  0.228221  0.053541    1.0   \n",
       "4       0.124955  0.154981  0.597647  0.013284  0.000454  0.234038    0.0   \n",
       "...          ...       ...       ...       ...       ...       ...    ...   \n",
       "145156  0.790734  0.878804  0.067239  0.082646  0.671506  0.455424    0.0   \n",
       "145157  0.801359  0.500603  0.069040  0.077637  0.327586  1.000000    0.0   \n",
       "145158  1.000000  0.752380  0.806268  0.250223  1.000000  0.462049    0.0   \n",
       "145159  1.000000  0.899182  0.362010  0.142534  0.086661  0.963140    0.0   \n",
       "145160  0.662786  0.307816  0.000000  0.373048  0.000454  0.110449    0.0   \n",
       "\n",
       "        b.Catenin  \n",
       "0        0.057827  \n",
       "1        0.267198  \n",
       "2        0.104686  \n",
       "3        0.283151  \n",
       "4        0.000000  \n",
       "...           ...  \n",
       "145156   0.040877  \n",
       "145157   0.068794  \n",
       "145158   0.086740  \n",
       "145159   0.167498  \n",
       "145160   0.000000  \n",
       "\n",
       "[145161 rows x 54 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/chl-codex-annotated/cHL_CODEX_annotation.csv\")\n",
    "\n",
    "# First, let's inspect what columns we actually have\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET INSPECTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumn names ({len(df.columns)} total):\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i:2d}. {col:40s} (dtype: {df[col].dtype})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIRST FEW ROWS:\")\n",
    "print(\"=\" * 80)\n",
    "pd.set_option('display.max_columns', None)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Individual cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:59:54.630678Z",
     "iopub.status.busy": "2026-01-16T15:59:54.630193Z",
     "iopub.status.idle": "2026-01-16T15:59:54.900746Z",
     "shell.execute_reply": "2026-01-16T15:59:54.899653Z",
     "shell.execute_reply.started": "2026-01-16T15:59:54.630641Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell Type Distribution ---\n",
      "cellType\n",
      "CD4              37480\n",
      "CD8              17184\n",
      "B                16196\n",
      "DC                9637\n",
      "Endothelial       8705\n",
      "Tumor             8260\n",
      "NK                7339\n",
      "M2                7286\n",
      "Monocyte          6913\n",
      "Other             5108\n",
      "Lymphatic         3768\n",
      "Neutrophil        3442\n",
      "TReg              3352\n",
      "Mast              3324\n",
      "M1                3101\n",
      "Epithelial        2251\n",
      "Seg Artifact      1431\n",
      "Cytotoxic CD8      384\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find target column - we know it's 'cellType' from the column list\n",
    "print(\"=\" * 80)\n",
    "print(\"FINDING TARGET COLUMN (Cell Type)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Based on your dataset, the target column is 'cellType'\n",
    "target_col = 'cellType'\n",
    "\n",
    "if target_col in df.columns:\n",
    "    print(f\"✅ Found target column: '{target_col}'\")\n",
    "else:\n",
    "    print(f\"❌ ERROR: Column '{target_col}' not found!\")\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "    raise ValueError(f\"Expected column '{target_col}' not in dataset\")\n",
    "\n",
    "# Show distribution\n",
    "counts = df[target_col].value_counts()\n",
    "print(f\"\\n--- Cell Type Distribution ({len(counts)} classes) ---\")\n",
    "for cell_type, count in counts.items():\n",
    "    print(f\"  {cell_type:30s}: {count:7,} cells ({100*count/len(df):5.2f}%)\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "counts.plot(kind='bar', ax=ax, color='steelblue', edgecolor='black', linewidth=1.5)\n",
    "ax.set_title(f\"Cell Type Distribution (Total: {len(df):,} cells)\", fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Cell Type', fontsize=13)\n",
    "ax.set_ylabel('Count', fontsize=13)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n💡 Target column: '{target_col}' with {len(counts)} cell types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN Implementation (Data Prep + Model + Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:59:54.904992Z",
     "iopub.status.busy": "2026-01-16T15:59:54.904590Z",
     "iopub.status.idle": "2026-01-16T15:59:55.304168Z",
     "shell.execute_reply": "2026-01-16T15:59:55.301833Z",
     "shell.execute_reply.started": "2026-01-16T15:59:54.904956Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Processing Data & Building Graph ---\n",
      "Features: 53 markers found.\n",
      "Coordinates: None, None\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: None",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_212/2864713414.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# 2. Labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Convert string labels to integers if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. DATA PREPARATION & GRAPH CONSTRUCTION\n",
    "# ==========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: DATA PREPARATION & GRAPH CONSTRUCTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- A. Define Column Names (Based on Your Dataset) ---\n",
    "# You provided these exact column names:\n",
    "x_col = 'X_cent'        # X coordinate\n",
    "y_col = 'Y_cent'        # Y coordinate  \n",
    "label_col = 'cellType'  # Cell type label (target)\n",
    "id_col = 'cellLabel'    # Cell ID\n",
    "size_col = 'cellSize'   # Cell size (morphological feature)\n",
    "\n",
    "# All protein markers (54 markers total)\n",
    "marker_cols = [\n",
    "    'BCL.2', 'CCR6', 'CD11b', 'CD11c', 'CD15', 'CD16', 'CD162', 'CD163', \n",
    "    'CD2', 'CD20', 'CD206', 'CD25', 'CD30', 'CD31', 'CD4', 'CD44', \n",
    "    'CD45RA', 'CD45RO', 'CD45', 'CD5', 'CD56', 'CD57', 'CD68', 'CD69', \n",
    "    'CD7', 'CD8', 'Collagen.4', 'Cytokeratin', 'DAPI.01', 'EGFR', \n",
    "    'FoxP3', 'Granzyme.B', 'HLA.DR', 'IDO.1', 'LAG.3', 'MCT', 'MMP.9', \n",
    "    'MUC.1', 'PD.1', 'PD.L1', 'Podoplanin', 'T.bet', 'TCR.g.d', 'TCRb', \n",
    "    'Tim.3', 'VISA', 'Vimentin', 'a.SMA', 'b.Catenin'\n",
    "]\n",
    "\n",
    "# Try to detect region column (for spatial split)\n",
    "region_candidates = ['Region', 'Image', 'Reg', 'Point', 'fov', 'FOV', 'region_id', 'image_id', 'Core', 'ImageId']\n",
    "region_col = None\n",
    "for col in region_candidates:\n",
    "    if col in df.columns:\n",
    "        region_col = col\n",
    "        break\n",
    "\n",
    "print(\"\\n--- Column Mapping ---\")\n",
    "print(f\"  X coordinate:     {x_col}\")\n",
    "print(f\"  Y coordinate:     {y_col}\")\n",
    "print(f\"  Cell type (target): {label_col}\")\n",
    "print(f\"  Cell ID:          {id_col}\")\n",
    "print(f\"  Cell size:        {size_col}\")\n",
    "print(f\"  Region/Image:     {region_col if region_col else 'NOT FOUND (will use random split)'}\")\n",
    "print(f\"  Protein markers:  {len(marker_cols)} markers\")\n",
    "\n",
    "# Verify all columns exist\n",
    "missing_cols = []\n",
    "for col in [x_col, y_col, label_col] + marker_cols:\n",
    "    if col not in df.columns:\n",
    "        missing_cols.append(col)\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"\\n❌ ERROR: Missing columns: {missing_cols}\")\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "    raise ValueError(\"Some required columns are missing from the dataset\")\n",
    "\n",
    "print(\"  ✅ All required columns found!\")\n",
    "\n",
    "# --- B. Feature Summary ---\n",
    "print(f\"\\n--- Dataset Summary ---\")\n",
    "print(f\"  Total cells:      {len(df):,}\")\n",
    "print(f\"  Features:         {len(marker_cols)} protein markers + 2 spatial coords\")\n",
    "print(f\"  Classes:          {df[label_col].nunique()} cell types\")\n",
    "print(f\"  First 10 markers: {marker_cols[:10]}\")\n",
    "\n",
    "# --- C. Create Feature Tensors ---\n",
    "print(f\"\\n--- Creating Tensors ---\")\n",
    "\n",
    "# 1. Normalize marker features (Z-score normalization)\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(df[marker_cols].values)\n",
    "x = torch.tensor(X_normalized, dtype=torch.float)\n",
    "print(f\"  Features tensor:  {x.shape} (normalized)\")\n",
    "\n",
    "# 2. Encode labels as integers\n",
    "unique_labels = sorted(df[label_col].unique())\n",
    "label_map = {name: i for i, name in enumerate(unique_labels)}\n",
    "y = torch.tensor(df[label_col].map(label_map).values, dtype=torch.long)\n",
    "num_classes = len(label_map)\n",
    "\n",
    "print(f\"\\n  Label Encoding ({num_classes} classes):\")\n",
    "for name, idx in sorted(label_map.items(), key=lambda x: x[1]):\n",
    "    count = (df[label_col] == name).sum()\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"    {idx:2d}. {name:30s} → {count:6,} cells ({pct:5.2f}%)\")\n",
    "\n",
    "print(f\"\\n  Labels tensor:    {y.shape}\")\n",
    "\n",
    "# --- D. Build KNN Graph from Spatial Coordinates ---\n",
    "print(f\"\\n--- Building KNN Spatial Graph ---\")\n",
    "k_neighbors = 5\n",
    "coords = df[[x_col, y_col]].values\n",
    "\n",
    "print(f\"  Coordinate matrix: {coords.shape}\")\n",
    "print(f\"  K-nearest neighbors: {k_neighbors}\")\n",
    "print(f\"  Building KNN graph...\")\n",
    "\n",
    "# Fit KNN (using ball_tree for efficiency)\n",
    "nbrs = NearestNeighbors(n_neighbors=k_neighbors + 1, algorithm='ball_tree').fit(coords)\n",
    "distances, indices = nbrs.kneighbors(coords)\n",
    "\n",
    "# Create edge list (exclude self-loops)\n",
    "source_nodes = np.repeat(np.arange(len(df)), k_neighbors)\n",
    "target_nodes = indices[:, 1:].flatten()  # Skip column 0 (self-connection)\n",
    "edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
    "\n",
    "print(f\"  ✅ Graph constructed!\")\n",
    "print(f\"  Total edges:       {edge_index.shape[1]:,}\")\n",
    "print(f\"  Average degree:    {edge_index.shape[1] / len(df):.2f}\")\n",
    "print(f\"  Edge sparsity:     {100 * edge_index.shape[1] / (len(df)**2):.4f}%\")\n",
    "\n",
    "# --- E. CRITICAL: Spatial Split (Prevent Data Leakage) ---\n",
    "print(f\"\\n--- Spatial Train/Test Split ---\")\n",
    "\n",
    "if region_col:\n",
    "    # Region-based split (RECOMMENDED for GNNs)\n",
    "    unique_regions = sorted(df[region_col].unique())\n",
    "    n_regions = len(unique_regions)\n",
    "    n_train_regions = int(0.8 * n_regions)\n",
    "    \n",
    "    # Use first 80% of regions for training\n",
    "    train_regions = unique_regions[:n_train_regions]\n",
    "    test_regions = unique_regions[n_train_regions:]\n",
    "    \n",
    "    train_mask = torch.tensor(df[region_col].isin(train_regions).values, dtype=torch.bool)\n",
    "    test_mask = torch.tensor(df[region_col].isin(test_regions).values, dtype=torch.bool)\n",
    "    \n",
    "    print(f\"  ✅ SPATIAL SPLIT (Region-based)\")\n",
    "    print(f\"  This prevents data leakage in GNN by ensuring:\")\n",
    "    print(f\"  - Training cells NEVER neighbor test cells\")\n",
    "    print(f\"  - Model tested on completely unseen tissue regions\")\n",
    "    print(f\"\\n  Split Details:\")\n",
    "    print(f\"  Total regions:     {n_regions}\")\n",
    "    print(f\"  Train regions:     {n_train_regions} → {train_mask.sum():,} cells ({100*train_mask.float().mean():.1f}%)\")\n",
    "    print(f\"  Test regions:      {len(test_regions)} → {test_mask.sum():,} cells ({100*test_mask.float().mean():.1f}%)\")\n",
    "    print(f\"  Train IDs:         {train_regions[:5]}{'...' if len(train_regions) > 5 else ''}\")\n",
    "    print(f\"  Test IDs:          {test_regions}\")\n",
    "else:\n",
    "    # Fallback to random split (NOT recommended for GNNs)\n",
    "    print(f\"  ⚠️  RANDOM SPLIT (Region column not found)\")\n",
    "    print(f\"  WARNING: This causes DATA LEAKAGE in GNN!\")\n",
    "    print(f\"  Test cells will have edges to training cells.\")\n",
    "    train_mask = torch.rand(len(df)) < 0.8\n",
    "    test_mask = ~train_mask\n",
    "    print(f\"\\n  Train: {train_mask.sum():,} cells ({100*train_mask.float().mean():.1f}%)\")\n",
    "    print(f\"  Test:  {test_mask.sum():,} cells ({100*test_mask.float().mean():.1f}%)\")\n",
    "\n",
    "# --- F. Create PyTorch Geometric Data Object ---\n",
    "print(f\"\\n--- Creating PyTorch Geometric Data Object ---\")\n",
    "data = Data(\n",
    "    x=x,\n",
    "    edge_index=edge_index,\n",
    "    y=y,\n",
    "    train_mask=train_mask,\n",
    "    test_mask=test_mask\n",
    ")\n",
    "\n",
    "print(f\"  ✅ Data object created!\")\n",
    "print(f\"\\n{data}\\n\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL DEFINITIONS\n",
    "# ==========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 2: MODEL DEFINITIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Baseline MLP (No spatial info - replicates MAPS approach)\n",
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Baseline: 4-layer MLP (MAPS approach)\n",
    "    Treats each cell independently - NO spatial neighborhood info.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        self.fc2 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.fc3 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.fc4 = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index=None):  # edge_index ignored (not used)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# GraphSAGE (Our hypothesis: spatial context improves classification)\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSAGE: Aggregates information from spatial neighbors.\n",
    "    Hypothesis: Cells with ambiguous markers can be classified better\n",
    "    by considering their neighborhood context.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Layer 1: Aggregate from immediate neighbors (1-hop)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Layer 2: Aggregate from 2-hop neighborhood\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "print(\"  ✅ MLP (Baseline - MAPS approach)\")\n",
    "print(\"     → Ignores spatial context\")\n",
    "print(\"     → 4 fully connected layers\")\n",
    "print(\"\\n  ✅ GraphSAGE (GNN approach)\")\n",
    "print(\"     → Uses spatial neighborhood\")\n",
    "print(\"     → 2 graph convolution layers\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. TRAINING FUNCTIONS\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: TRAINING SETUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Training function (works for both MLP and GNN)\n",
    "def train_epoch(model, data, optimizer, use_loader=False, loader=None):\n",
    "    model.train()\n",
    "    \n",
    "    if use_loader:  # Mini-batch training for GNN (memory-efficient)\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "            # Loss only on center nodes (not the sampled neighbors)\n",
    "            loss = F.nll_loss(out[:batch.batch_size], batch.y[:batch.batch_size])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(loader)\n",
    "    else:  # Full-batch training for MLP\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "# Evaluation function (calculates accuracy and F1)\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, mask):\n",
    "    model.eval()\n",
    "    data_device = data.to(device)\n",
    "    out = model(data_device.x, data_device.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    \n",
    "    # Move back to CPU for sklearn metrics\n",
    "    y_true = data_device.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = (pred[mask] == data_device.y[mask]).sum().item() / mask.sum().item()\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    return acc, f1, y_true, y_pred\n",
    "\n",
    "print(\"  ✅ Training functions defined\")\n",
    "print(\"  ✅ Evaluation functions defined\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAIN BASELINE MLP\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: TRAINING BASELINE MLP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "hidden_dim = 128\n",
    "dropout = 0.1  # MAPS uses 0.1\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "\n",
    "mlp_model = MLP(\n",
    "    in_channels=len(marker_cols),\n",
    "    hidden_channels=hidden_dim,\n",
    "    out_channels=num_classes,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "mlp_optimizer = torch.optim.Adam(mlp_model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "\n",
    "print(f\"\\nHyperparameters:\")\n",
    "print(f\"  Input features:   {len(marker_cols)}\")\n",
    "print(f\"  Hidden dimension: {hidden_dim}\")\n",
    "print(f\"  Output classes:   {num_classes}\")\n",
    "print(f\"  Dropout:          {dropout}\")\n",
    "print(f\"  Learning rate:    {lr}\")\n",
    "print(f\"  Epochs:           {epochs}\")\n",
    "\n",
    "print(f\"\\nTraining MLP...\")\n",
    "print(\"Epoch | Loss    | Train Acc | Train F1 | Test Acc | Test F1\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "best_mlp_f1 = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train_epoch(mlp_model, data, mlp_optimizer, use_loader=False)\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        train_acc, train_f1, _, _ = evaluate(mlp_model, data, data.train_mask)\n",
    "        test_acc, test_f1, _, _ = evaluate(mlp_model, data, data.test_mask)\n",
    "        \n",
    "        if test_f1 > best_mlp_f1:\n",
    "            best_mlp_f1 = test_f1\n",
    "        \n",
    "        print(f'{epoch:5d} | {loss:7.4f} | {train_acc:9.4f} | {train_f1:8.4f} | '\n",
    "              f'{test_acc:8.4f} | {test_f1:7.4f}')\n",
    "\n",
    "print(f\"\\n✅ MLP Training Complete!\")\n",
    "print(f\"   Best Test F1: {best_mlp_f1:.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. TRAIN GRAPHSAGE (Memory-Efficient)\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: TRAINING GRAPHSAGE (GNN)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gnn_model = GraphSAGE(\n",
    "    in_channels=len(marker_cols),\n",
    "    hidden_channels=hidden_dim,\n",
    "    out_channels=num_classes,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "gnn_optimizer = torch.optim.Adam(gnn_model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "\n",
    "# NeighborLoader: Memory-efficient mini-batch training\n",
    "# This is CRITICAL for fitting on your 4GB GPU!\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[10, 10],  # Sample 10 neighbors at each layer\n",
    "    batch_size=2048,         # Process 2048 cells per batch\n",
    "    input_nodes=data.train_mask,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"\\nNeighborLoader Configuration:\")\n",
    "print(f\"  Batch size:       2048 cells\")\n",
    "print(f\"  Neighbor sampling: [10, 10] (10 neighbors per layer)\")\n",
    "print(f\"  Total batches:    {len(train_loader)} per epoch\")\n",
    "print(f\"  ✅ This prevents GPU OOM on your 4GB GTX 1650 Ti!\")\n",
    "\n",
    "print(f\"\\nTraining GraphSAGE...\")\n",
    "print(\"Epoch | Loss    | Train Acc | Train F1 | Test Acc | Test F1\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "best_gnn_f1 = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train_epoch(gnn_model, data, gnn_optimizer, use_loader=True, loader=train_loader)\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        train_acc, train_f1, _, _ = evaluate(gnn_model, data, data.train_mask)\n",
    "        test_acc, test_f1, _, _ = evaluate(gnn_model, data, data.test_mask)\n",
    "        \n",
    "        if test_f1 > best_gnn_f1:\n",
    "            best_gnn_f1 = test_f1\n",
    "        \n",
    "        print(f'{epoch:5d} | {loss:7.4f} | {train_acc:9.4f} | {train_f1:8.4f} | '\n",
    "              f'{test_acc:8.4f} | {test_f1:7.4f}')\n",
    "\n",
    "print(f\"\\n✅ GraphSAGE Training Complete!\")\n",
    "print(f\"   Best Test F1: {best_gnn_f1:.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. COMPARISON & RESULTS\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL RESULTS: MLP vs GraphSAGE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Final evaluation\n",
    "mlp_test_acc, mlp_test_f1, mlp_y_true, mlp_y_pred = evaluate(mlp_model, data, data.test_mask)\n",
    "gnn_test_acc, gnn_test_f1, gnn_y_true, gnn_y_pred = evaluate(gnn_model, data, data.test_mask)\n",
    "\n",
    "print(f\"\\n{'Model':<20s} {'Accuracy':>12s} {'F1-Score':>12s} {'vs MAPS (90%)':>15s}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'MLP (Baseline)':<20s} {mlp_test_acc:>12.4f} {mlp_test_f1:>12.4f} {mlp_test_f1/0.90:>14.2f}x\")\n",
    "print(f\"{'GraphSAGE (GNN)':<20s} {gnn_test_acc:>12.4f} {gnn_test_f1:>12.4f} {gnn_test_f1/0.90:>14.2f}x\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "improvement = ((gnn_test_f1 - mlp_test_f1) / mlp_test_f1) * 100\n",
    "print(f\"\\n📊 GNN vs MLP Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "if gnn_test_f1 > mlp_test_f1:\n",
    "    print(f\"✅ SUCCESS! GNN outperforms MLP by {improvement:.2f}%\")\n",
    "    print(\"   → Spatial neighborhood context HELPS classification!\")\n",
    "elif gnn_test_f1 > 0.90:\n",
    "    print(f\"✅ SUCCESS! GNN beats MAPS baseline (90%)!\")\n",
    "    print(f\"   → Your approach achieves {gnn_test_f1:.1%} F1!\")\n",
    "else:\n",
    "    print(f\"🤔 GNN didn't beat MLP. Possible reasons:\")\n",
    "    print(\"   - May need more epochs or hyperparameter tuning\")\n",
    "    print(\"   - Spatial context might not be strongly predictive for this dataset\")\n",
    "    print(\"   - Try adjusting K neighbors, hidden dim, or dropout\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart comparison\n",
    "ax = axes[0]\n",
    "models = ['MLP\\n(Baseline)', 'GraphSAGE\\n(GNN)']\n",
    "f1_scores = [mlp_test_f1, gnn_test_f1]\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "\n",
    "bars = ax.bar(models, f1_scores, color=colors, edgecolor='black', linewidth=2, width=0.6)\n",
    "ax.set_ylabel('F1-Score', fontsize=14, fontweight='bold')\n",
    "ax.set_title('MLP vs GraphSAGE: Cell Type Classification', fontsize=16, fontweight='bold')\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.axhline(y=0.90, color='green', linestyle='--', linewidth=2, label='MAPS Baseline (90%)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.4f}',\n",
    "            ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Metrics comparison\n",
    "ax2 = axes[1]\n",
    "metrics = ['Accuracy', 'F1-Score']\n",
    "mlp_metrics = [mlp_test_acc, mlp_test_f1]\n",
    "gnn_metrics = [gnn_test_acc, gnn_test_f1]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, mlp_metrics, width, label='MLP', color='#3498db', edgecolor='black', linewidth=1.5)\n",
    "bars2 = ax2.bar(x + width/2, gnn_metrics, width, label='GraphSAGE', color='#e74c3c', edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax2.set_ylabel('Score', fontsize=14, fontweight='bold')\n",
    "ax2.set_title('Performance Metrics Comparison', fontsize=16, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(metrics, fontsize=12)\n",
    "ax2.set_ylim([0, 1.0])\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🚀 ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n🎯 Your Research Goal: Beat MAPS (90% F1) using spatial context\")\n",
    "print(f\"   MLP Result:      {mlp_test_f1:.1%}\")\n",
    "print(f\"   GraphSAGE Result: {gnn_test_f1:.1%}\")\n",
    "if gnn_test_f1 > 0.90:\n",
    "    print(f\"   ✅ HYPOTHESIS CONFIRMED! 🎉\")\n",
    "else:\n",
    "    print(f\"   ⚠️  More tuning needed to beat 90%\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9273088,
     "sourceId": 14519119,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
